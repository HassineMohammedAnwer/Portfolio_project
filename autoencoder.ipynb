{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import keras.backend as K  # Import Keras backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "df_customers = pd.read_csv('data/prepared_data/customers.csv')\n",
    "df_orders = pd.read_csv('data/prepared_data/orders.csv')\n",
    "df_products = pd.read_csv('data/prepared_data/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lineitem_sku</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NEG0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lineitem_sku\n",
       "count           402\n",
       "unique          402\n",
       "top         NEG0015\n",
       "freq              1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products.drop(['published'], axis=1, inplace=True)\n",
    "df_products.rename(columns={'variant_sku': 'lineitem_sku'}, inplace=True)\n",
    "df_products.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lineitem_sku</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>120422</td>\n",
       "      <td>120422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>388</td>\n",
       "      <td>53899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>LOT200001A106/000-BLC-TU</td>\n",
       "      <td>'4491371249860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4104</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    lineitem_sku     customer_id\n",
       "count                     120422          120422\n",
       "unique                       388           53899\n",
       "top     LOT200001A106/000-BLC-TU  '4491371249860\n",
       "freq                        4104            1978"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78902, 123)\n",
      "(120422, 14)\n",
      "(402, 84)\n"
     ]
    }
   ],
   "source": [
    "print(df_customers.shape)\n",
    "print(df_orders.shape)\n",
    "print(df_products.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lineitem_sku    BBU2AMI01A009/000-CHR-3XL  BBU2AMI01A009/000-CHR-LXL  \\\n",
      "customer_id                                                            \n",
      "'3220287651979                        0.0                        1.0   \n",
      "'4464852369604                        0.0                        0.0   \n",
      "'4470765650116                        0.0                        0.0   \n",
      "'4471456465092                        0.0                        0.0   \n",
      "'4471462068420                        0.0                        0.0   \n",
      "\n",
      "lineitem_sku    BBU2AMI01A009/000-CHR-SM  BBU2AMI01A009/000-CHR-XXL  \\\n",
      "customer_id                                                           \n",
      "'3220287651979                       0.0                        0.0   \n",
      "'4464852369604                       0.0                        0.0   \n",
      "'4470765650116                       0.0                        0.0   \n",
      "'4471456465092                       0.0                        0.0   \n",
      "'4471462068420                       0.0                        0.0   \n",
      "\n",
      "lineitem_sku    BBU2AMI01A009/000-NOI-3XL  BBU2AMI01A009/000-NOI-LXL  \\\n",
      "customer_id                                                            \n",
      "'3220287651979                        0.0                        0.0   \n",
      "'4464852369604                        0.0                        0.0   \n",
      "'4470765650116                        0.0                        0.0   \n",
      "'4471456465092                        0.0                        0.0   \n",
      "'4471462068420                        0.0                        0.0   \n",
      "\n",
      "lineitem_sku    BBU2AMI01A009/000-NOI-SM  BBU2AMI01A009/000-NOI-XXL  \\\n",
      "customer_id                                                           \n",
      "'3220287651979                       0.0                        0.0   \n",
      "'4464852369604                       0.0                        0.0   \n",
      "'4470765650116                       0.0                        0.0   \n",
      "'4471456465092                       0.0                        0.0   \n",
      "'4471462068420                       0.0                        0.0   \n",
      "\n",
      "lineitem_sku    BDD2RAF01A085/000-NOI-LXL  BDD2RAF01A085/000-NOI-SM  ...  \\\n",
      "customer_id                                                          ...   \n",
      "'3220287651979                        0.0                       0.0  ...   \n",
      "'4464852369604                        0.0                       0.0  ...   \n",
      "'4470765650116                        0.0                       0.0  ...   \n",
      "'4471456465092                        0.0                       0.0  ...   \n",
      "'4471462068420                        0.0                       0.0  ...   \n",
      "\n",
      "lineitem_sku    TSH1AMI01A107/000-NOI-M  TSH1AMI01A107/000-NOI-XL  \\\n",
      "customer_id                                                         \n",
      "'3220287651979                      0.0                       0.0   \n",
      "'4464852369604                      0.0                       0.0   \n",
      "'4470765650116                      0.0                       0.0   \n",
      "'4471456465092                      0.0                       0.0   \n",
      "'4471462068420                      0.0                       0.0   \n",
      "\n",
      "lineitem_sku    TSH1DEC04A109/001-NOI-LXL  TSH1DEC04A109/001-NOI-SM  \\\n",
      "customer_id                                                           \n",
      "'3220287651979                        0.0                       0.0   \n",
      "'4464852369604                        0.0                       0.0   \n",
      "'4470765650116                        0.0                       0.0   \n",
      "'4471456465092                        0.0                       0.0   \n",
      "'4471462068420                        0.0                       0.0   \n",
      "\n",
      "lineitem_sku    TSM1AMI01A064/000-BLC-L  TSM1AMI01A064/000-BLC-M  \\\n",
      "customer_id                                                        \n",
      "'3220287651979                      0.0                      0.0   \n",
      "'4464852369604                      0.0                      0.0   \n",
      "'4470765650116                      0.0                      0.0   \n",
      "'4471456465092                      0.0                      0.0   \n",
      "'4471462068420                      0.0                      0.0   \n",
      "\n",
      "lineitem_sku    TSM1AMI01A064/000-BLC-XL  TSM1AMI01A064/000-NOI-L  \\\n",
      "customer_id                                                         \n",
      "'3220287651979                       0.0                      0.0   \n",
      "'4464852369604                       0.0                      0.0   \n",
      "'4470765650116                       0.0                      0.0   \n",
      "'4471456465092                       0.0                      0.0   \n",
      "'4471462068420                       0.0                      0.0   \n",
      "\n",
      "lineitem_sku    TSM1AMI01A064/000-NOI-M  TSM1AMI01A064/000-NOI-XL  \n",
      "customer_id                                                        \n",
      "'3220287651979                      0.0                       0.0  \n",
      "'4464852369604                      0.0                       0.0  \n",
      "'4470765650116                      0.0                       0.0  \n",
      "'4471456465092                      0.0                       0.0  \n",
      "'4471462068420                      0.0                       0.0  \n",
      "\n",
      "[5 rows x 388 columns]\n"
     ]
    }
   ],
   "source": [
    "# aggregate duplicate entries by summing lineitem_quantity\n",
    "df_orders_aggregated = df_orders.groupby(['customer_id', 'lineitem_sku'], as_index=False).agg({'lineitem_quantity': 'sum'})\n",
    "\n",
    "# create user-item interaction matrix (pivot table)\n",
    "user_item_matrix = df_orders_aggregated.pivot(index='customer_id', columns='lineitem_sku', values='lineitem_quantity').fillna(0)\n",
    "\n",
    "print(user_item_matrix.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1348/1348 [==============================] - 9s 5ms/step - loss: 0.0877 - val_loss: 0.0473\n",
      "Epoch 2/50\n",
      "1348/1348 [==============================] - 6s 5ms/step - loss: 0.0819 - val_loss: 0.0447\n",
      "Epoch 3/50\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.0805 - val_loss: 0.0439\n",
      "Epoch 4/50\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.0800 - val_loss: 0.0438\n",
      "Epoch 5/50\n",
      "1348/1348 [==============================] - 9s 7ms/step - loss: 0.0800 - val_loss: 0.0438\n",
      "Epoch 6/50\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.0799 - val_loss: 0.0436\n",
      "Epoch 7/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0797 - val_loss: 0.0435\n",
      "Epoch 8/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0796 - val_loss: 0.0434\n",
      "Epoch 9/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0795 - val_loss: 0.0432\n",
      "Epoch 10/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0793 - val_loss: 0.0430\n",
      "Epoch 11/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0791 - val_loss: 0.0429\n",
      "Epoch 12/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0790 - val_loss: 0.0429\n",
      "Epoch 13/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0790 - val_loss: 0.0427\n",
      "Epoch 14/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0789 - val_loss: 0.0427\n",
      "Epoch 15/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0788 - val_loss: 0.0426\n",
      "Epoch 16/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0788 - val_loss: 0.0426\n",
      "Epoch 17/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0787 - val_loss: 0.0425\n",
      "Epoch 18/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0786 - val_loss: 0.0424\n",
      "Epoch 19/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0786 - val_loss: 0.0424\n",
      "Epoch 20/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0786 - val_loss: 0.0424\n",
      "Epoch 21/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0786 - val_loss: 0.0425\n",
      "Epoch 22/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0786 - val_loss: 0.0423\n",
      "Epoch 23/50\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.0785 - val_loss: 0.0423\n",
      "Epoch 24/50\n",
      "1348/1348 [==============================] - 7s 6ms/step - loss: 0.0785 - val_loss: 0.0423\n",
      "Epoch 25/50\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.0785 - val_loss: 0.0423\n",
      "Epoch 26/50\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.0785 - val_loss: 0.0423\n",
      "Epoch 27/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0785 - val_loss: 0.0424\n",
      "Epoch 28/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 29/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0784 - val_loss: 0.0423\n",
      "Epoch 30/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 31/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0784 - val_loss: 0.0423\n",
      "Epoch 32/50\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.0784 - val_loss: 0.0423\n",
      "Epoch 33/50\n",
      "1348/1348 [==============================] - 9s 7ms/step - loss: 0.0784 - val_loss: 0.0423\n",
      "Epoch 34/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 35/50\n",
      "1348/1348 [==============================] - 6s 5ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 36/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 37/50\n",
      "1348/1348 [==============================] - 6s 4ms/step - loss: 0.0784 - val_loss: 0.0423\n",
      "Epoch 38/50\n",
      "1348/1348 [==============================] - 6s 4ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 39/50\n",
      "1348/1348 [==============================] - 10s 8ms/step - loss: 0.0784 - val_loss: 0.0423\n",
      "Epoch 40/50\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 41/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 42/50\n",
      "1348/1348 [==============================] - 6s 5ms/step - loss: 0.0784 - val_loss: 0.0423\n",
      "Epoch 43/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 44/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0783 - val_loss: 0.0422\n",
      "Epoch 45/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 46/50\n",
      "1348/1348 [==============================] - 12s 9ms/step - loss: 0.0783 - val_loss: 0.0421\n",
      "Epoch 47/50\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.0783 - val_loss: 0.0422\n",
      "Epoch 48/50\n",
      "1348/1348 [==============================] - 9s 6ms/step - loss: 0.0784 - val_loss: 0.0422\n",
      "Epoch 49/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0783 - val_loss: 0.0422\n",
      "Epoch 50/50\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.0783 - val_loss: 0.0423\n",
      "1685/1685 [==============================] - 6s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test = train_test_split(user_item_matrix.values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the number of input features (number of products)\n",
    "n_inputs = user_item_matrix.shape[1]\n",
    "\n",
    "# Autoencoder Architecture\n",
    "input_layer = Input(shape=(n_inputs,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)  # Encoder\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "latent = Dense(32, activation='relu')(encoded)  # Latent Space\n",
    "\n",
    "decoded = Dense(64, activation='relu')(latent)  # Decoder\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "output_layer = Dense(n_inputs, activation='sigmoid')(decoded)  # Reconstructed Output\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Use the trained model to make predictions\n",
    "reconstructed = autoencoder.predict(user_item_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.1032\n",
      "Recall@10: 0.5851\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have reconstructed interactions from your autoencoder\n",
    "predicted_interactions = reconstructed\n",
    "actual_interactions = user_item_matrix.values\n",
    "\n",
    "# Function to evaluate Precision@k and Recall@k\n",
    "def evaluate_model(predictions, actuals, k=10):\n",
    "    # Convert predictions to binary interactions (1 if interacted, else 0) by thresholding\n",
    "    top_k_preds = predictions.argsort(axis=1)[:, -k:]\n",
    "    \n",
    "    # Initialize lists to collect per user precision and recall\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    # Iterate over each user row\n",
    "    for i in range(actuals.shape[0]):\n",
    "        # Get actual positive interactions\n",
    "        actual_set = set([idx for idx, value in enumerate(actuals[i]) if value > 0])\n",
    "\n",
    "        # Get predicted top-k interactions\n",
    "        pred_set = set(top_k_preds[i])\n",
    "\n",
    "        # Calculate precision and recall for this user\n",
    "        true_positives = len(actual_set & pred_set)\n",
    "        precision = true_positives / k\n",
    "        recall = true_positives / len(actual_set) if actual_set else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    # Average precision and recall\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "\n",
    "    print(f'Precision@{k}: {avg_precision:.4f}')\n",
    "    print(f'Recall@{k}: {avg_recall:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(predicted_interactions, actual_interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick: z = mu + sigma * epsilon\"\"\"\n",
    "    mu, log_sigma = args\n",
    "    batch = keras.backend.shape(mu)[0]\n",
    "    dim = keras.backend.shape(mu)[1]\n",
    "    epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
    "    return mu + keras.backend.exp(log_sigma / 2) * epsilon\n",
    "\n",
    "def autoencoder(input_dims, hidden_layers, latent_dims):\n",
    "    \"\"\"\n",
    "    Creates a Variational Autoencoder (VAE).\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    x = Input(shape=(input_dims,))\n",
    "    hidden = Dense(hidden_layers[0], activation='relu')(x)\n",
    "    for units in hidden_layers[1:]:\n",
    "        hidden = Dense(units, activation='relu')(hidden)\n",
    "\n",
    "\n",
    "    # Latent space\n",
    "    z_mean = Dense(latent_dims, activation=None)(hidden)\n",
    "    z_log_sigma = Dense(latent_dims, activation=None)(hidden)\n",
    "    z = Lambda(sampling, output_shape=(latent_dims,))([z_mean, z_log_sigma])\n",
    "\n",
    "    encoder = Model(x, [z, z_mean, z_log_sigma], name=\"encoder\")\n",
    "\n",
    "    # Decoder\n",
    "    latent_inputs = Input(shape=(latent_dims,))\n",
    "    hidden_dec = Dense(hidden_layers[-1], activation='relu')(latent_inputs)\n",
    "    for units in reversed(hidden_layers[:-1]):\n",
    "        hidden_dec = Dense(units, activation='relu')(hidden_dec)\n",
    "\n",
    "    outputs = Dense(input_dims, activation='sigmoid')(hidden_dec)\n",
    "    decoder = Model(latent_inputs, outputs, name=\"decoder\")\n",
    "\n",
    "    # VAE Model\n",
    "    vae_outputs = decoder(encoder(x)[0])\n",
    "    vae = Model(x, vae_outputs, name=\"vae\")\n",
    "\n",
    "    # Compile VAE\n",
    "    vae.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')\n",
    "    \n",
    "    return encoder, decoder, vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the user-item interaction matrix\n",
    "# Assuming `user_item_matrix` is created correctly from your data\n",
    "X = user_item_matrix.values\n",
    "input_dims = X.shape[1]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model parameters\n",
    "hidden_layers = [128, 64]  # Number of nodes in hidden layers\n",
    "latent_dims = 32  # Size of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1348/1348 [==============================] - 11s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/50\n",
      "1348/1348 [==============================] - 9s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/50\n",
      "1348/1348 [==============================] - 9s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/50\n",
      "1348/1348 [==============================] - 9s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/50\n",
      "1348/1348 [==============================] - 9s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/50\n",
      "1348/1348 [==============================] - 10s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/50\n",
      "1348/1348 [==============================] - 9s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/50\n",
      "1348/1348 [==============================] - 16s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/50\n",
      "1348/1348 [==============================] - 12s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/50\n",
      "1348/1348 [==============================] - 12s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/50\n",
      "1348/1348 [==============================] - 14s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/50\n",
      "1348/1348 [==============================] - 9s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/50\n",
      "1348/1348 [==============================] - 9s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/50\n",
      "1348/1348 [==============================] - 9s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 21/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 22/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 23/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 24/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 25/50\n",
      "1348/1348 [==============================] - 12s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 26/50\n",
      "1348/1348 [==============================] - 15s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 27/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 28/50\n",
      "1348/1348 [==============================] - 11s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 29/50\n",
      "1348/1348 [==============================] - 12s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 30/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 31/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/50\n",
      "1348/1348 [==============================] - 10s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "1348/1348 [==============================] - 12s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "1348/1348 [==============================] - 13s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "1348/1348 [==============================] - 13s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "1348/1348 [==============================] - 10s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 41/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "1348/1348 [==============================] - 11s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "1348/1348 [==============================] - 10s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "1348/1348 [==============================] - 10s 7ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x234800f68d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the VAE\n",
    "encoder, decoder, vae = autoencoder(input_dims, hidden_layers, latent_dims)\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(X_train, X_train, \n",
    "        epochs=50, \n",
    "        batch_size=32, \n",
    "        validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1685/1685 [==============================] - 6s 3ms/step\n",
      "1685/1685 [==============================] - 6s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use the encoder and decoder for predictions\n",
    "encoded_data = encoder.predict(X)  # Encoded user representations\n",
    "reconstructed_data = vae.predict(X)  # Reconstructed interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the reconstructed predictions and actual values (threshold = 0.5 for interaction)\n",
    "predicted_interactions = (reconstructed > 0.5).astype(int)\n",
    "actual_interactions = (user_item_matrix.values > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.9740\n",
      "Recall@10: 0.9740\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate precision@K and recall@K\n",
    "def precision_recall_at_k(predicted, actual, k=10):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    for user_idx in range(len(actual)):\n",
    "        # Get the top-k predictions for each user\n",
    "        top_k_indices = np.argsort(-predicted[user_idx])[:k]  # Sort and take the indices of the top k items\n",
    "        top_k_predicted = np.zeros_like(predicted[user_idx])\n",
    "        top_k_predicted[top_k_indices] = 1  # Set top k items as 1\n",
    "\n",
    "        # Extract actual interactions\n",
    "        actual_k = actual[user_idx]\n",
    "\n",
    "        # Calculate precision and recall using average='binary' to handle individual classes\n",
    "        precision = precision_score(actual_k, top_k_predicted, average='micro', zero_division=0)\n",
    "        recall = recall_score(actual_k, top_k_predicted, average='micro', zero_division=0)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "    # Averaging precision and recall across all users\n",
    "    avg_precision = np.mean(precision_list)\n",
    "    avg_recall = np.mean(recall_list)\n",
    "\n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "# Evaluate precision@10 and recall@10\n",
    "precision_at_10, recall_at_10 = precision_recall_at_k(predicted_interactions, actual_interactions, k=10)\n",
    "\n",
    "print(f'Precision@10: {precision_at_10:.4f}')\n",
    "print(f'Recall@10: {recall_at_10:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "excellent results! A precision and recall of 0.9712 at k=10\n",
    "Variational Autoencoder (VAE) model is effectively identifying relevant recommendations. Hereâ€™s a breakdown of what this means and what you might consider next:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'latent_dim': [16, 32, 64],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'l2_reg': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [50, 100]\n",
    "}\n",
    "\n",
    "# Generate a random sample of hyperparameters to try\n",
    "param_samples = list(ParameterSampler(param_grid, n_iter=10, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the interaction matrix into training and test sets at the interaction level\n",
    "train_mask = np.random.rand(*user_item_matrix.shape) < 0.8\n",
    "train_matrix = np.multiply(user_item_matrix.values, train_mask)  # Training data\n",
    "test_matrix = np.multiply(user_item_matrix.values, ~train_mask)  # Test data (hidden during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input size\n",
    "n_inputs = user_item_matrix.shape[1]\n",
    "\n",
    "best_precision = 0\n",
    "best_model = None\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params: {'learning_rate': 0.001, 'latent_dim': 64, 'l2_reg': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'batch_size': 32}\n",
      "1685/1685 [==============================] - 5s 3ms/step\n",
      "Precision@10: 0.0000, Recall@10: 0.0000\n",
      "Testing params: {'learning_rate': 0.001, 'latent_dim': 16, 'l2_reg': 0.001, 'epochs': 50, 'dropout_rate': 0.3, 'batch_size': 32}\n",
      "1685/1685 [==============================] - 5s 3ms/step\n",
      "Precision@10: 0.0016, Recall@10: 0.0144\n",
      "Testing params: {'learning_rate': 0.0001, 'latent_dim': 16, 'l2_reg': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'batch_size': 32}\n",
      "1685/1685 [==============================] - 5s 3ms/step\n",
      "Precision@10: 0.0016, Recall@10: 0.0142\n",
      "Testing params: {'learning_rate': 0.001, 'latent_dim': 16, 'l2_reg': 0.01, 'epochs': 50, 'dropout_rate': 0.1, 'batch_size': 32}\n",
      "1685/1685 [==============================] - 5s 3ms/step\n",
      "Precision@10: 0.0000, Recall@10: 0.0000\n",
      "Testing params: {'learning_rate': 0.001, 'latent_dim': 16, 'l2_reg': 0.1, 'epochs': 50, 'dropout_rate': 0.1, 'batch_size': 64}\n",
      "1685/1685 [==============================] - 5s 3ms/step\n",
      "Precision@10: 0.0000, Recall@10: 0.0000\n",
      "Testing params: {'learning_rate': 0.001, 'latent_dim': 16, 'l2_reg': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'batch_size': 32}\n",
      "1685/1685 [==============================] - 6s 3ms/step\n",
      "Precision@10: 0.0000, Recall@10: 0.0000\n",
      "Testing params: {'learning_rate': 0.001, 'latent_dim': 32, 'l2_reg': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'batch_size': 64}\n",
      "1685/1685 [==============================] - 5s 3ms/step\n",
      "Precision@10: 0.0017, Recall@10: 0.0158\n",
      "Testing params: {'learning_rate': 0.0005, 'latent_dim': 64, 'l2_reg': 0.001, 'epochs': 100, 'dropout_rate': 0.1, 'batch_size': 64}\n",
      "1685/1685 [==============================] - 6s 4ms/step\n",
      "Precision@10: 0.0012, Recall@10: 0.0109\n",
      "Testing params: {'learning_rate': 0.0001, 'latent_dim': 32, 'l2_reg': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'batch_size': 64}\n",
      "1685/1685 [==============================] - 5s 3ms/step\n",
      "Precision@10: 0.0000, Recall@10: 0.0000\n",
      "Testing params: {'learning_rate': 0.001, 'latent_dim': 16, 'l2_reg': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'batch_size': 32}\n",
      "1685/1685 [==============================] - 6s 3ms/step\n",
      "Precision@10: 0.0000, Recall@10: 0.0000\n",
      "Best Precision@10: 0.0017 with params: {'learning_rate': 0.001, 'latent_dim': 32, 'l2_reg': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Hyperparameter tuning loop\n",
    "for params in param_samples:\n",
    "    print(f\"Testing params: {params}\")\n",
    "\n",
    "    # Build the autoencoder model with the current hyperparameters\n",
    "    input_layer = Input(shape=(n_inputs,))\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(params['l2_reg']))(input_layer)\n",
    "    x = Dropout(params['dropout_rate'])(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(params['l2_reg']))(x)\n",
    "    x = Dropout(params['dropout_rate'])(x)\n",
    "    latent = Dense(params['latent_dim'], activation='relu', kernel_regularizer=l2(params['l2_reg']))(x)\n",
    "\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(params['l2_reg']))(latent)\n",
    "    x = Dropout(params['dropout_rate'])(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(params['l2_reg']))(x)\n",
    "    output_layer = Dense(n_inputs, activation='sigmoid')(x)\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=params['learning_rate']),\n",
    "                        loss=BinaryCrossentropy())\n",
    "\n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Train the model using only the training data\n",
    "    history = autoencoder.fit(train_matrix, train_matrix,\n",
    "                              epochs=params['epochs'],\n",
    "                              batch_size=params['batch_size'],\n",
    "                              validation_split=0.1,\n",
    "                              shuffle=True,\n",
    "                              callbacks=[early_stopping],\n",
    "                              verbose=0)\n",
    "\n",
    "    # Predict interactions using the trained model\n",
    "    predicted_interactions = autoencoder.predict(user_item_matrix.values)\n",
    "\n",
    "    # Function to evaluate Precision@k and Recall@k on the test interactions\n",
    "    def evaluate_model(predictions, actuals, test_mask, k=10):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "\n",
    "        # Apply a threshold to binarize the predictions\n",
    "        binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "        # Iterate over each user row\n",
    "        for i in range(actuals.shape[0]):\n",
    "            # Get actual positive interactions from the test set\n",
    "            actual_set = set([idx for idx, value in enumerate(actuals[i]) if value > 0 and test_mask[i, idx]])\n",
    "\n",
    "            # Get predicted top-k interactions\n",
    "            top_k_indices = np.argsort(-predictions[i])[:k]\n",
    "            pred_set = set([idx for idx in top_k_indices if binary_predictions[i, idx] > 0])\n",
    "\n",
    "            # Calculate precision and recall for this user\n",
    "            true_positives = len(actual_set & pred_set)\n",
    "            precision = true_positives / k if k else 0\n",
    "            recall = true_positives / len(actual_set) if actual_set else 0\n",
    "\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "\n",
    "        # Average precision and recall across all users\n",
    "        avg_precision = np.mean(precisions)\n",
    "        avg_recall = np.mean(recalls)\n",
    "\n",
    "        return avg_precision, avg_recall\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    precision_at_10, recall_at_10 = evaluate_model(predicted_interactions, actuals=user_item_matrix.values, test_mask=~train_mask, k=10)\n",
    "\n",
    "    print(f\"Precision@10: {precision_at_10:.4f}, Recall@10: {recall_at_10:.4f}\")\n",
    "\n",
    "    # Update best model if performance improves\n",
    "    if precision_at_10 > best_precision:\n",
    "        best_precision = precision_at_10\n",
    "        best_model = autoencoder\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Precision@10: {best_precision:.4f} with params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params: {'learning_rate': 0.001, 'latent_dim': 64, 'l2_reg': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'batch_size': 32}\n",
      "1685/1685 [==============================] - 6s 3ms/step\n",
      "Precision@10: 0.0000, Recall@10: 0.0000\n",
      "Testing params: {'learning_rate': 0.001, 'latent_dim': 16, 'l2_reg': 0.001, 'epochs': 50, 'dropout_rate': 0.3, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "# Apply PCA to reduce dimensionality of the user-item matrix\n",
    "n_components = 50  # Adjust based on the desired level of dimensionality reduction\n",
    "pca = PCA(n_components=n_components)\n",
    "user_item_matrix_pca = pca.fit_transform(user_item_matrix.values)\n",
    "\n",
    "# Split transformed interaction matrix into train and test sets\n",
    "train_mask = np.random.rand(*user_item_matrix_pca.shape) < 0.8\n",
    "train_matrix = np.multiply(user_item_matrix_pca, train_mask)\n",
    "test_matrix = np.multiply(user_item_matrix_pca, ~train_mask)\n",
    "\n",
    "# Parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'latent_dim': [16, 32, 64],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'l2_reg': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [50, 100]\n",
    "}\n",
    "\n",
    "# Generate a random sample of hyperparameters to try\n",
    "param_samples = list(ParameterSampler(param_grid, n_iter=10, random_state=42))\n",
    "\n",
    "# Define the input size after PCA\n",
    "n_inputs = user_item_matrix_pca.shape[1]\n",
    "\n",
    "# Track the best model performance\n",
    "best_precision = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "# Hyperparameter tuning loop\n",
    "for params in param_samples:\n",
    "    print(f\"Testing params: {params}\")\n",
    "\n",
    "    # Build the autoencoder model with the current hyperparameters\n",
    "    input_layer = Input(shape=(n_inputs,))\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(params['l2_reg']))(input_layer)\n",
    "    x = Dropout(params['dropout_rate'])(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(params['l2_reg']))(x)\n",
    "    x = Dropout(params['dropout_rate'])(x)\n",
    "    latent = Dense(params['latent_dim'], activation='relu', kernel_regularizer=l2(params['l2_reg']))(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(params['l2_reg']))(latent)\n",
    "    x = Dropout(params['dropout_rate'])(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(params['l2_reg']))(x)\n",
    "    output_layer = Dense(n_inputs, activation='sigmoid')(x)\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=params['learning_rate']), loss=BinaryCrossentropy())\n",
    "\n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Train the model using only the training data\n",
    "    history = autoencoder.fit(\n",
    "        train_matrix, train_matrix,\n",
    "        epochs=params['epochs'],\n",
    "        batch_size=params['batch_size'],\n",
    "        validation_split=0.1,\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predict interactions using the trained model\n",
    "    predicted_interactions = autoencoder.predict(user_item_matrix_pca)\n",
    "\n",
    "    # Function to evaluate Precision@k and Recall@k on the test interactions\n",
    "    def evaluate_model(predictions, actuals, test_mask, k=10):\n",
    "        precisions, recalls = [], []\n",
    "\n",
    "        # Binarize predictions with a threshold\n",
    "        binary_predictions = (predictions > 0.5).astype(int)\n",
    "        for i in range(actuals.shape[0]):\n",
    "            actual_set = set([idx for idx, value in enumerate(actuals[i]) if value > 0 and test_mask[i, idx]])\n",
    "            top_k_indices = np.argsort(-predictions[i])[:k]\n",
    "            pred_set = set([idx for idx in top_k_indices if binary_predictions[i, idx] > 0])\n",
    "\n",
    "            true_positives = len(actual_set & pred_set)\n",
    "            precision = true_positives / k if k else 0\n",
    "            recall = true_positives / len(actual_set) if actual_set else 0\n",
    "\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "\n",
    "        return np.mean(precisions), np.mean(recalls)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    precision_at_10, recall_at_10 = evaluate_model(predicted_interactions, user_item_matrix_pca, ~train_mask, k=10)\n",
    "\n",
    "    print(f\"Precision@10: {precision_at_10:.4f}, Recall@10: {recall_at_10:.4f}\")\n",
    "\n",
    "    # Track the best performing model\n",
    "    if precision_at_10 > best_precision:\n",
    "        best_precision = precision_at_10\n",
    "        best_model = autoencoder\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Precision@10: {best_precision:.4f} with params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
